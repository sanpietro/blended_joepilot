{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tiktoken\n",
    "\n",
    "# Add your own OpenAI API key\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15' # this may change in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = Path(\"YOUR_TEXT\").read_text()\n",
    "print(source_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length=1024):\n",
    "    chunks = []\n",
    "    words = text.split()\n",
    "    current_chunk = \"\"\n",
    "    for word in words:\n",
    "        if len(current_chunk) + len(word) < max_length:\n",
    "            current_chunk += f\" {word}\"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = f\"{word}\"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):\n",
    "    input_chunks = split_text(text)\n",
    "    output_chunks = []\n",
    "    for chunk in input_chunks:\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"YOUR_DEPLOYMENT_NAME\",\n",
    "            prompt=(\"This text is a chunked summary of a transcript. Create a coherent summary based on the text. Avoid repeating names and do not mention when participants join or leave the meeting\" + \"\\n\\n\" + f\"{chunk}\"),\n",
    "            temperature=0.5,\n",
    "            max_tokens=1024,\n",
    "            n = 1,\n",
    "            stop=None\n",
    "        )\n",
    "        summary = response.choices[0].text.strip()\n",
    "        output_chunks.append(summary)\n",
    "    return \" \".join(output_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_of_summaries(first_summary_text):\n",
    "    first_summary_text = generate_summary(first_summary_text)\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"YOUR_DEPLOYMENT_NAME\",\n",
    "        prompt=(\"This text is a chunked summary of summarized text. Create a coherent 500-1000 word blended summary of the key points based on the text. Do not mention transcript. Write as if you are describing what was discussed. Discuss the topics themselves and not who discussed them.\" + \"\\n\\n\" + f\"{first_summary_text}\"),\n",
    "            temperature=0.5,\n",
    "            max_tokens=1024,\n",
    "            n = 1,\n",
    "            stop=None,\n",
    "        \n",
    "    )\n",
    "    summary_of_summary = response['choices'][0]['text']\n",
    "    return summary_of_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = summary_of_summaries(source_text)\n",
    "print(responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
